{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Extracting YT comments, title and description using YT data API\n",
    "\n",
    "# Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from googleapiclient.discovery import build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API to fetch the news details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variables\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# YouTube category ID mapping (API does not support violence and explicit)\n",
    "CATEGORY_IDS = {\n",
    "    \"gaming\": \"20\",\n",
    "    \"news\": \"25\",\n",
    "    \"entertainment\": \"24\",\n",
    "    \"sports\": \"17\"\n",
    "}\n",
    "SEARCH_CATEGORIES = [\"education\", \"violence\", \"explicit/18+\"]  # Use search for these\n",
    "\n",
    "# List of explicit/unsafe keywords\n",
    "VIOLENCE_KEYWORDS = [\"violence\", \"war\", \"murder\", \"gun\", \"blood\", \"kill\", \"crime\", \"assault\", \"shooting\", \"attack\"]\n",
    "EXPLICIT_KEYWORDS = [\"explicit\", \"nsfw\", \"mature\", \"adult\", \"nude\", \"porn\", \"sex\", \"drugs\", \"abuse\", \"18+\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing the content into different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_content(title, description, tags):\n",
    "    \"\"\"Categorize videos as 'violence', 'explicit/18+', or normal.\"\"\"\n",
    "    combined_text = f\"{title.lower()} {description.lower()} {' '.join(tags).lower()}\"\n",
    "    if any(word in combined_text for word in EXPLICIT_KEYWORDS):\n",
    "        return \"explicit/18+\"\n",
    "    elif any(word in combined_text for word in VIOLENCE_KEYWORDS):\n",
    "        return \"violence\"\n",
    "    else:\n",
    "        return \"normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the first 10 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(video_id, max_comments=10):\n",
    "    \"\"\"Fetch top comments for a given video.\"\"\"\n",
    "    comments = []\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_comments,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "    except Exception:\n",
    "        return \"Comments Disabled\"\n",
    "    return \" | \".join(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the top 15 trending videos according to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trending_videos(category_id, category_name, max_results=15):\n",
    "    \"\"\"Fetch trending videos in a given category (Trending API).\"\"\"\n",
    "    try:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=\"US\",\n",
    "            videoCategoryId=category_id,\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        video_data = []\n",
    "        for item in response.get(\"items\", []):\n",
    "            video_id = item[\"id\"]\n",
    "            title = item[\"snippet\"][\"title\"]\n",
    "            description = item[\"snippet\"][\"description\"]\n",
    "            views = item[\"statistics\"][\"viewCount\"]\n",
    "            likes = item[\"statistics\"].get(\"likeCount\", \"0\")\n",
    "            tags = item[\"snippet\"].get(\"tags\", [])\n",
    "            \n",
    "            # Categorization\n",
    "            category_final = categorize_content(title, description, tags)\n",
    "            comments = get_video_comments(video_id, max_comments=15)\n",
    "\n",
    "            video_data.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"views\": views,\n",
    "                \"likes\": likes,\n",
    "                \"comments\": comments,\n",
    "                \"explicit\": \"yes\" if category_final == \"explicit/18+\" else \"no\",\n",
    "                \"violent\": \"yes\" if category_final == \"violence\" else \"no\",\n",
    "                \"category\": category_final if category_final != \"normal\" else category_name\n",
    "            })\n",
    "\n",
    "        return video_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching trending videos for category {category_name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search videos for specific categories for which tags aren't available by YT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_videos(query, category_name, max_results=15):\n",
    "    \"\"\"Search for videos instead of fetching trending (for Education, Explicit, Violence).\"\"\"\n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            type=\"video\",\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        video_data = []\n",
    "        for item in response.get(\"items\", []):\n",
    "            video_id = item[\"id\"][\"videoId\"]\n",
    "            title = item[\"snippet\"][\"title\"]\n",
    "            description = item[\"snippet\"][\"description\"]\n",
    "            tags = item[\"snippet\"].get(\"tags\", [])\n",
    "            \n",
    "            category_final = categorize_content(title, description, tags)\n",
    "            comments = get_video_comments(video_id, max_comments=10)\n",
    "\n",
    "            video_data.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"views\": \"N/A\",  # Search API doesn't return views\n",
    "                \"likes\": \"N/A\",  # Search API doesn't return likes\n",
    "                \"comments\": comments,\n",
    "                \"explicit\": \"yes\" if category_final == \"explicit/18+\" else \"no\",\n",
    "                \"violent\": \"yes\" if category_final == \"violence\" else \"no\",\n",
    "                \"category\": category_final if category_final != \"normal\" else category_name\n",
    "            })\n",
    "\n",
    "        return video_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error searching videos for category {category_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch videos\n",
    "all_videos = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get trending videos from supported categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, cat_id in CATEGORY_IDS.items():\n",
    "    logging.info(f\"Fetching trending videos for category: {category}\")\n",
    "    videos = get_trending_videos(cat_id, category, max_results=15)\n",
    "    all_videos.extend(videos)\n",
    "\n",
    "# Get searched videos for Education, Violence, Explicit\n",
    "for category in SEARCH_CATEGORIES:\n",
    "    logging.info(f\"Searching videos for category: {category}\")\n",
    "    videos = search_videos(category, category, max_results=15)\n",
    "    all_videos.extend(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n",
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n",
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        video_id                                              title  \\\n",
      "0    B9ou3pu3xSQ  SIDEMEN AMONG US DRAFT MODE BUT EVERYONE CHOOS...   \n",
      "1    PRaLwvY4SoM            I Spent $9,713 at a Card Shop in Roblox   \n",
      "2    e2w2flW5_q4          Making Money Moves (Schedule 1 Episode 2)   \n",
      "3    RbYEN9vAe5M  ã€MINECRAFT RP #NIJIEnchantedã€‘TRYING TO DO COOL...   \n",
      "4    2rifjk558yM            ã€BLOODBORNEã€‘Kos, or as some say, Koseki   \n",
      "..           ...                                                ...   \n",
      "139  E9xsVnDz0uw  Can&#39;t Be Real! Did Crockett PROMOTE VIOLEN...   \n",
      "140  3pCeKF2qhaE  Man Arrested At Airport After Suspected Domest...   \n",
      "141  nkPdMQCizIQ                   å¥³çŽ‹èœ‚ã€Žãƒã‚¤ã‚ªãƒ¬ãƒ³ã‚¹(VIOLENCE)ã€Official MV   \n",
      "142  B8Jbse2CoSU  Nagpur Curfew Lifted After Mob Violence Reduce...   \n",
      "143  EX_8ZjT2sO4  Grenouer - Alone in the Dark - [UNCENSORED - A...   \n",
      "\n",
      "                                           description    views   likes  \\\n",
      "0    ðŸ—: Order food NOW at: https://www.eatsides.com...  2286244  111110   \n",
      "1    We play a Card Shop Simulator in Roblox\\n\\nFri...   505619   17659   \n",
      "2    Join this channel to get access to uploads wit...   347799   12302   \n",
      "3    Luca Luca Daisuki ã ã„ã™ããƒ¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\\n\\n ...    93553    2558   \n",
      "4    awOOOOoooOOOOOO\\n\\nThumbnail art: @decrilus TH...   381134   14098   \n",
      "..                                                 ...      ...     ...   \n",
      "139  Join this channel to get access to perks: http...      N/A     N/A   \n",
      "140  A 79 -year-old woman has been stabbed to death...      N/A     N/A   \n",
      "141  TVã‚¢ãƒ‹ãƒ¡ã€Žãƒã‚§ãƒ³ã‚½ãƒ¼ãƒžãƒ³ã€ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ†ãƒ¼ãƒžã€Žãƒã‚¤ã‚ªãƒ¬ãƒ³ã‚¹(VIOLENCE)ã€ Str...      N/A     N/A   \n",
      "142  Seven days after mob violence and arson in Nag...      N/A     N/A   \n",
      "143  Official music video for 'Alone in the Dark' f...      N/A     N/A   \n",
      "\n",
      "                                              comments explicit violent  \\\n",
      "0    21:00 I'M ROLLING! | Deji gotta buy a quieter ...       no      no   \n",
      "1    19:10 LIAR | can u play free fire | I swear Bi...       no      no   \n",
      "2    47:46 this is BY FAR one of the funniest thing...       no      no   \n",
      "3    the loading screen is so cute!! otsu boss for ...       no      no   \n",
      "4    Fell asleep in the middle, but shoutout to Bij...       no     yes   \n",
      "..                                                 ...      ...     ...   \n",
      "139  OMG ! Crocket is horrible !! | You can take th...       no     yes   \n",
      "140                                  Comments Disabled       no     yes   \n",
      "141  ã€Žãƒã‚¤ã‚ªãƒ¬ãƒ³ã‚¹(VIOLENCE)ã€\\r\\n\\r\\nã‚ã‚ã¾ãŸæ­£å¤¢ã°ã‹ã‚Š\\r\\næµ®è…«ã‚“ã ç›®ã‚’è¦‹...       no     yes   \n",
      "142  India gave zero to world & India has \\nZero ac...       no     yes   \n",
      "143  .. | Aur bhai gaana bhi dekh le ðŸŒš | ðŸ˜‚ | 0.50 |...       no     yes   \n",
      "\n",
      "     category  \n",
      "0      gaming  \n",
      "1      gaming  \n",
      "2      gaming  \n",
      "3      gaming  \n",
      "4    violence  \n",
      "..        ...  \n",
      "139  violence  \n",
      "140  violence  \n",
      "141  violence  \n",
      "142  violence  \n",
      "143  violence  \n",
      "\n",
      "[144 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(all_videos)\n",
    "\n",
    "# Separate explicit and violence videos\n",
    "explicit_videos = df[df[\"explicit\"] == \"yes\"]\n",
    "violent_videos = df[df[\"violent\"] == \"yes\"]\n",
    "\n",
    "# Add explicit and violent videos separately\n",
    "df = pd.concat([df, explicit_videos.assign(category=\"explicit/18+\")], ignore_index=True)\n",
    "df = pd.concat([df, violent_videos.assign(category=\"violence\")], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"trending_yt_vids.csv\", index=False)\n",
    "logging.info(\"Data saved to trending_yt_vids.csv\")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing the data stored in the data frame\n",
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from better_profanity import profanity\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Custom explicit word blacklist since yt doesn't have tags for explicit/18+ videos\n",
    "custom_blacklist = {\"violence\", \"drugs\", \"kill\", \"murder\", \"terrorist\", \"sex\", \"nude\", \"scam\", \"gun\", \"assault\",\"explicit\",\"xxx\",\"porn\",\"18+\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply various preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return \"\"\n",
    "    \n",
    "    # Expand contractions (e.g., \"can't\" â†’ \"cannot\", \"I'm\" â†’ \"I am\")\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    \n",
    "    # Remove special characters, punctuation, and extra spaces\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens))]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Function to classify sentiment as Safe or Harmful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']  # Get compound sentiment score\n",
    "    \n",
    "    return \"Safe\" if sentiment_score >= -0.4 else \"Harmful\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to detect explicit content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_explicit_content(text):\n",
    "    # Check if the text contains explicit words\n",
    "    if profanity.contains_profanity(text) or any(word in text.lower() for word in custom_blacklist):\n",
    "        return \"Harmful\"\n",
    "    return \"Safe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to classify video as Safe or Harmful based on sentiment & explicit content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_video(row):\n",
    "    # If any text contains \"18+\" or \"explicit\", classify immediately as Harmful\n",
    "    if any(\"18+\" in row[col].lower() or \"explicit\" in row[col].lower() or \"sex\" in row[col].lower() for col in [\"title\", \"description\", \"comments\"]):\n",
    "        return \"Harmful\"\n",
    "\n",
    "    sentiments = [row[\"title_sentiment\"], row[\"description_sentiment\"], row[\"comments_sentiment\"]]\n",
    "    explicit_flags = [row[\"title_explicit\"], row[\"description_explicit\"], row[\"comments_explicit\"]]\n",
    "\n",
    "    # If at least 3 elements are Harmful (either sentiment OR explicit content), classify as Harmful\n",
    "    if sentiments.count(\"Harmful\") + explicit_flags.count(\"Harmful\") >= 3:\n",
    "        return \"Harmful\"\n",
    "\n",
    "    return \"Safe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV file (data collected from YouTube API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Saved as 'processed_youtube_data.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP\\trending_yt_vids.csv\")\n",
    "\n",
    "# Preprocess text data\n",
    "for col in [\"title\", \"description\", \"comments\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "df[\"title_sentiment\"] = df[\"title\"].apply(classify_sentiment)\n",
    "df[\"description_sentiment\"] = df[\"description\"].apply(classify_sentiment)\n",
    "df[\"comments_sentiment\"] = df[\"comments\"].apply(classify_sentiment)\n",
    "\n",
    "# Detect explicit content\n",
    "df[\"title_explicit\"] = df[\"title\"].apply(detect_explicit_content)\n",
    "df[\"description_explicit\"] = df[\"description\"].apply(detect_explicit_content)\n",
    "df[\"comments_explicit\"] = df[\"comments\"].apply(detect_explicit_content)\n",
    "\n",
    "# Classify video as Safe or Harmful\n",
    "df[\"video_classification\"] = df.apply(classify_video, axis=1)\n",
    "\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"preprocessed_yt_data.csv\", index=False)\n",
    "\n",
    "print(\"Processing complete. Saved as 'processed_youtube_data.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
