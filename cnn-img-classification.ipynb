{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11146081,"sourceType":"datasetVersion","datasetId":6953288},{"sourceId":11146848,"sourceType":"datasetVersion","datasetId":6953869}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import the necessary libraries**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:59:40.329311Z","iopub.execute_input":"2025-03-24T09:59:40.329613Z","iopub.status.idle":"2025-03-24T09:59:40.333964Z","shell.execute_reply.started":"2025-03-24T09:59:40.329590Z","shell.execute_reply":"2025-03-24T09:59:40.332793Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# **Apply data augmentation technics and load the images**","metadata":{}},{"cell_type":"code","source":"train_dir = \"/kaggle/input/data-cnn/data_cnn/train\"\nval_dir = \"/kaggle/input/data-cnn/data_cnn/val\"\ntest_dir = \"/kaggle/input/data-cnn/data_cnn/test\"\n\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,rotation_range=30,width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\nval_test_datagen = ImageDataGenerator(rescale=1./255,rotation_range=30,width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\nval_generator = val_test_datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\ntest_generator = val_test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:59:40.364928Z","iopub.execute_input":"2025-03-24T09:59:40.365124Z","iopub.status.idle":"2025-03-24T09:59:50.844942Z","shell.execute_reply.started":"2025-03-24T09:59:40.365107Z","shell.execute_reply":"2025-03-24T09:59:50.844218Z"}},"outputs":[{"name":"stdout","text":"Found 7602 images belonging to 2 classes.\nFound 1628 images belonging to 2 classes.\nFound 1632 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(\"Train classes:\", train_generator.class_indices)\nprint(\"Validation classes:\", val_generator.class_indices)\nprint(\"Test classes:\", test_generator.class_indices)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:59:50.846456Z","iopub.execute_input":"2025-03-24T09:59:50.846700Z","iopub.status.idle":"2025-03-24T09:59:50.852334Z","shell.execute_reply.started":"2025-03-24T09:59:50.846669Z","shell.execute_reply":"2025-03-24T09:59:50.851360Z"}},"outputs":[{"name":"stdout","text":"Train classes: {'safe': 0, 'unsafe': 1}\nValidation classes: {'safe': 0, 'unsafe': 1}\nTest classes: {'safe': 0, 'unsafe': 1}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# **Build Model**","metadata":{}},{"cell_type":"code","source":"base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Freeze pretrained layers\n\n\nmodel = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:04:44.687416Z","iopub.execute_input":"2025-03-24T10:04:44.687700Z","iopub.status.idle":"2025-03-24T10:04:45.664355Z","shell.execute_reply.started":"2025-03-24T10:04:44.687677Z","shell.execute_reply":"2025-03-24T10:04:45.663671Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m4,049,571\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m327,936\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,410,532\u001b[0m (16.82 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,410,532</span> (16.82 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m360,961\u001b[0m (1.38 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">360,961</span> (1.38 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:05:12.485010Z","iopub.execute_input":"2025-03-24T10:05:12.485356Z","iopub.status.idle":"2025-03-24T10:05:12.494857Z","shell.execute_reply.started":"2025-03-24T10:05:12.485327Z","shell.execute_reply":"2025-03-24T10:05:12.493951Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# **Train Model**","metadata":{}},{"cell_type":"code","source":"\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=10,  # Increased epochs, but will stop early if overfitting\n    callbacks=[early_stopping, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:05:15.333459Z","iopub.execute_input":"2025-03-24T10:05:15.333767Z","iopub.status.idle":"2025-03-24T10:13:46.854819Z","shell.execute_reply.started":"2025-03-24T10:05:15.333743Z","shell.execute_reply":"2025-03-24T10:13:46.853806Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 554ms/step - accuracy: 0.4868 - loss: 0.6954 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 489ms/step - accuracy: 0.4874 - loss: 0.6942 - val_accuracy: 0.5184 - val_loss: 0.6932 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m237/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.5127 - loss: 0.6930\nEpoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 493ms/step - accuracy: 0.5128 - loss: 0.6930 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 489ms/step - accuracy: 0.5145 - loss: 0.6933 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 5.0000e-05\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# **Unfreeze last 30 layers and finetune the model acc to our custom dataset**","metadata":{}},{"cell_type":"code","source":"\nfor layer in base_model.layers[:-30]:  \n    layer.trainable = False\n\n# Recompile with a very low learning rate\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\n\n# Use Early Stopping & Learning Rate Scheduler\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n\n# Fine-tune the model\nhistory_fine = model.fit(train_generator, validation_data=val_generator, \n                         epochs=20, callbacks=[early_stopping, lr_scheduler])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:04.414403Z","iopub.execute_input":"2025-03-24T10:15:04.414739Z","iopub.status.idle":"2025-03-24T10:23:35.331252Z","shell.execute_reply.started":"2025-03-24T10:15:04.414711Z","shell.execute_reply":"2025-03-24T10:23:35.330547Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 549ms/step - accuracy: 0.5019 - loss: 0.6934 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 1.0000e-05\nEpoch 2/20\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 491ms/step - accuracy: 0.5227 - loss: 0.6922 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 1.0000e-05\nEpoch 3/20\n\u001b[1m237/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 0.5140 - loss: 0.6924\nEpoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 490ms/step - accuracy: 0.5139 - loss: 0.6924 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 1.0000e-05\nEpoch 4/20\n\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 491ms/step - accuracy: 0.5065 - loss: 0.6926 - val_accuracy: 0.5184 - val_loss: 0.6925 - learning_rate: 5.0000e-06\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# **Test on the test set**","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_generator)\nprint(f\"Test Accuracy: {test_acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:23:46.851306Z","iopub.execute_input":"2025-03-24T10:23:46.851598Z","iopub.status.idle":"2025-03-24T10:24:11.254653Z","shell.execute_reply.started":"2025-03-24T10:23:46.851574Z","shell.execute_reply":"2025-03-24T10:24:11.253937Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 461ms/step - accuracy: 0.1783 - loss: 0.7231\nTest Accuracy: 51.84%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model.save(\"/kaggle/working/safe_unsafe_efficientnet.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:24:31.671524Z","iopub.execute_input":"2025-03-24T10:24:31.671805Z","iopub.status.idle":"2025-03-24T10:24:31.951965Z","shell.execute_reply.started":"2025-03-24T10:24:31.671781Z","shell.execute_reply":"2025-03-24T10:24:31.951224Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel = load_model(\"/kaggle/working/safe_unsafe_efficientnet.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:18:24.811843Z","iopub.execute_input":"2025-03-24T11:18:24.812159Z","iopub.status.idle":"2025-03-24T11:18:25.788912Z","shell.execute_reply.started":"2025-03-24T11:18:24.812133Z","shell.execute_reply":"2025-03-24T11:18:25.788014Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"# **Test the model on a completely new unseen dataset to see if the model generalizes well on images**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nnew_data_gen = ImageDataGenerator(rescale=1./255)  # Ensure same preprocessing\n\nnew_data = new_data_gen.flow_from_directory(\n    \"/kaggle/input/cnn-data-test/Child Safety.v2i.folder (1)/valid\",  \n    target_size=(224, 224),  # Use same size as training\n    batch_size=32, \n    class_mode=\"binary\"  # Use same class mode as training\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:18:28.338568Z","iopub.execute_input":"2025-03-24T11:18:28.338850Z","iopub.status.idle":"2025-03-24T11:18:28.356628Z","shell.execute_reply.started":"2025-03-24T11:18:28.338829Z","shell.execute_reply":"2025-03-24T11:18:28.355977Z"}},"outputs":[{"name":"stdout","text":"Found 512 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(new_data)\nprint(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:18:30.764498Z","iopub.execute_input":"2025-03-24T11:18:30.764775Z","iopub.status.idle":"2025-03-24T11:18:38.491346Z","shell.execute_reply.started":"2025-03-24T11:18:30.764754Z","shell.execute_reply":"2025-03-24T11:18:38.490587Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 115ms/step - accuracy: 0.5144 - loss: 0.6928\nTest Accuracy: 50.39%\n","output_type":"stream"}],"execution_count":50}]}